{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of user given seconds\n",
    "sc = SparkContext()\n",
    "\n",
    "batch_interval = 10\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "ssc.checkpoint(\"dgim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definition according to: http://infolab.stanford.edu/~ullman/mmds/ch4.pdf?fbclid=IwAR00gMaMWSqUsEAlfAZciQcj75dDFeeNI-Vs47aOcnp5Xd53Dn_3sqwMtls (Page 152)\n",
    "class Bucket:\n",
    "    def __init__(self, ts, ones):\n",
    "        self.ones = ones\n",
    "        self.final_timestamp = ts\n",
    "\n",
    "    # We use this func to merge two buckets and its ones count    \n",
    "    def __add__(self, second_bucket):\n",
    "        self.final_timestamp = max(self.final_timestamp, second_bucket.final_timestamp)\n",
    "        self.ones += second_bucket.ones\n",
    "        return self\n",
    "\n",
    "# queue struct to merge buckets\n",
    "class Queue:\n",
    "    def __init__(self):\n",
    "        self.buckets = [[]]\n",
    "\n",
    "    # push a bucket to the stack\n",
    "    def push(self, bucket):\n",
    "        self.buckets[0].insert(0, bucket)\n",
    "        \n",
    "        # merge buckets\n",
    "        for i in range(len(self.buckets)):\n",
    "            if len(self.buckets[i]) > 2:\n",
    "                try:\n",
    "                    self.buckets[i + 1].insert(0, self.buckets[i].pop() + self.buckets[i].pop())\n",
    "                except:\n",
    "                    # We need to compensate for the index error\n",
    "                    self.buckets.append([])\n",
    "                    self.buckets[i + 1].insert(0, self.buckets[i].pop() + self.buckets[i].pop())\n",
    "\n",
    "    def evaluate(self, end_ts):\n",
    "        ones = 0\n",
    "        last_bucket = 0\n",
    "\n",
    "        for bucket1 in self.buckets:\n",
    "            for bucket in bucket1:\n",
    "                # if its the last timestamp, we can get half of its ones count\n",
    "                if bucket.final_timestamp < end_ts:\n",
    "                    break\n",
    "                else:\n",
    "                    ones += bucket.ones\n",
    "                    last_bucket = bucket.ones\n",
    "\n",
    "        ones += math.floor(last_bucket / 2)\n",
    "\n",
    "        return ones\n",
    "\n",
    "# Visual improvement for development\n",
    "def quiet_logging(context):\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "\n",
    "# http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.DStream.updateStateByKey.html\n",
    "def dgim(incoming_stream, prev_stream):\n",
    "    samples = []\n",
    "    queue = Queue()\n",
    "\n",
    "    # resets every new stream\n",
    "    timestamp = 0\n",
    "    for elem in incoming_stream:\n",
    "        if elem=='1':\n",
    "            queue.push(Bucket(timestamp, 1))\n",
    "\n",
    "        timestamp += 1\n",
    "\n",
    "    # Searching for same given values: []\n",
    "    window_size = 10\n",
    "    result = queue.evaluate(timestamp - window_size)\n",
    "    samples.append((window_size, result))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def getOrderedCounts(rdd):\n",
    "    # We grab the two lists, and then map them into a better struct (value, weight)\n",
    "    counts_dict = rdd.flatMap(lambda x: x[1])    \n",
    "    return counts_dict\n",
    "\n",
    "# Examples taken from: http://spark.apache.org/docs/latest/streaming-programming-guide.html\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    quiet_logging(sc)\n",
    "\n",
    "    # Create a DStream that will connect to the data file given\n",
    "    # We need to run the program and only then, insert the files we want to count locations into this directory\n",
    "    lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "    # Split each line into pairs (Timestamp, location)\n",
    "    pairs = lines.map(lambda line: line)\n",
    "\n",
    "    # We always use the same key so we can update by key\n",
    "    pre_sampled_data = pairs.map(lambda bit: (0,bit)) \\\n",
    "\n",
    "    sampled_data = pre_sampled_data.updateStateByKey(dgim)\n",
    "\n",
    "    ordered_counts = sampled_data.transform(getOrderedCounts)\n",
    "\n",
    "    print(\"\\nEvaluating Queries: (window, number_of_ones)\")\n",
    "    ordered_counts.pprint(5)\n",
    "\n",
    "    ssc.start()             # start the computation\n",
    "    ssc.awaitTermination()  # wait for the computation to terminate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "third-mdle-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
