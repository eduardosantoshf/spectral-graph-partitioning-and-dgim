{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import math\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "batch_interval = 10 # batch interval of user given seconds\n",
    "\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "ssc.checkpoint(\"dgim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bucket:\n",
    "    def __init__(self, ts, ones):\n",
    "        self.ones = ones\n",
    "        self.final_ts = ts\n",
    "\n",
    "    # merge two buckets and its ones count \n",
    "    def __add__(self, second_bucket):\n",
    "        self.final_ts = max(self.final_ts, second_bucket.final_ts)\n",
    "        self.ones += second_bucket.ones\n",
    "\n",
    "        return self\n",
    "\n",
    "# queue struct to merge buckets\n",
    "class Queue:\n",
    "    def __init__(self):\n",
    "        self.buckets = [[]]\n",
    "\n",
    "    def push(self, bucket):\n",
    "        self.buckets[0].insert(0, bucket)\n",
    "        self._merge_buckets()\n",
    "\n",
    "    def _merge_buckets(self):\n",
    "        for i in range(len(self.buckets)):\n",
    "            if len(self.buckets[i]) > 2:\n",
    "                try:\n",
    "                    # merge the last two buckets in the current bucket group\n",
    "                    merged_bucket = self.buckets[i].pop() + self.buckets[i].pop()\n",
    "                    # insert the merged bucket at the beginning of the next bucket group\n",
    "                    self.buckets[i + 1].insert(0, merged_bucket)\n",
    "\n",
    "                except IndexError:\n",
    "                    # if an IndexError occurs, it means there is no next bucket group\n",
    "                    self.buckets.append([])\n",
    "                    # create a new empty bucket group and insert the merged bucket \n",
    "                    # at the beginning\n",
    "                    self.buckets[i + 1].insert(0, merged_bucket)\n",
    "\n",
    "    def evaluate(self, end_ts):\n",
    "        ones = 0\n",
    "        last_bucket = 0\n",
    "\n",
    "        for bucket_group in self.buckets:\n",
    "            for bucket in bucket_group:\n",
    "                # check if the final timestamp of the current bucket is less \n",
    "                # than the given end timestamp\n",
    "                if bucket.final_ts < end_ts:\n",
    "                    # exit the inner loop as we reached a bucket that is older \n",
    "                    # than the end timestamp\n",
    "                    break \n",
    "                else:\n",
    "                    ones += bucket.ones\n",
    "                    last_bucket = bucket.ones\n",
    "\n",
    "        # add half of the ones count in the last bucket \n",
    "        # (compensating for window size)\n",
    "        ones += math.floor(last_bucket / 2)\n",
    "\n",
    "        return ones\n",
    "\n",
    "\n",
    "def quiet_logging(context):\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "\n",
    "def dgim(incoming_stream, prev_stream):\n",
    "    samples = []\n",
    "    queue = Queue()\n",
    "\n",
    "    # resets every new stream\n",
    "    timestamp = 0\n",
    "    real_number_of_ones = 0\n",
    "\n",
    "    for elem in incoming_stream:\n",
    "        if elem == \"1\":\n",
    "            real_number_of_ones += 1\n",
    "\n",
    "            # create a new bucket with the current timestamp and add it to the queue\n",
    "            queue.push(Bucket(timestamp, 1))\n",
    "\n",
    "        timestamp += 1\n",
    "\n",
    "    window_size = 10\n",
    "\n",
    "    # evaluate the queue for the specified time window\n",
    "    number_of_ones = queue.evaluate(timestamp - window_size) \n",
    "\n",
    "    # append the collected sample to the list\n",
    "    samples.append((window_size, number_of_ones, real_number_of_ones))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def get_ordered_counts(rdd):\n",
    "    # map the two lists into a struct (value, weight)\n",
    "    counts_dict = rdd.flatMap(lambda x: x[1])    \n",
    "\n",
    "    return counts_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    quiet_logging(sc)\n",
    "\n",
    "    # Create a DStream by reading from a socket\n",
    "    lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "    # split each line into pairs (timestamp, position)\n",
    "    pairs = lines.map(lambda line: line)\n",
    "\n",
    "    pre_sampled_data = pairs.map(lambda bit: (0, bit))\n",
    "\n",
    "    # update the state of the DStream using dgim function\n",
    "    sampled_data = pre_sampled_data.updateStateByKey(dgim)\n",
    "\n",
    "    ordered_counts = sampled_data.transform(get_ordered_counts)\n",
    "\n",
    "    print(\"\\nResult: (window, number_of_ones)\")\n",
    "    ordered_counts.pprint(5)\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "third-mdle-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
